## MySQL 遇到过死锁问题吗，你是如何解决的？

```
查看死锁日志 show engine innodb status; 
找出死锁 Sql 
分析 sql 加锁情况 
模拟死锁案发 
分析死锁日志 
分析死锁结果
```

参考

-   [手把手教你分析Mysql死锁问题](https://mp.weixin.qq.com/s?__biz=Mzg3NzU5NTIwNg==&mid=2247487979&idx=1&sn=588c83d77a8851f3b3c18cd68ed9c454&chksm=cf21cec2f85647d4a77cc239ae9a4cfd31bb8832be3d98540a08ea8b4a1f46b38cf736210a02&token=1495321435&lang=zh_CN&scene=21#wechat_redirect)
-   [两万字详解！InnoDB锁专题！](https://mp.weixin.qq.com/s?__biz=Mzg3NzU5NTIwNg==&mid=2247499275&idx=1&sn=ca72f48a290e4fd2a2ded6ef6fd045be&chksm=cf222122f855a8347b911352cebdd722b17ea45733b91ff169353c0805d9f31cea5261ef01b9&token=1712314640&lang=zh_CN#rd)





## 日常工作中你是怎么优化 SQL 的？

```
加索引 
避免返回不必要的数据
适当分批量进行 
优化 sql 结构 
分库分表 
读写分离
```

参考

-   [后端程序员必备：书写高质量SQL的30条建议](https://mp.weixin.qq.com/s?__biz=Mzg3NzU5NTIwNg==&mid=2247487972&idx=1&sn=cd035a7fcd7496658846ab9f914be2db&chksm=cf21cecdf85647dbc53e212bf1a2b95d0eb2bffe08dc0141e01f8a9b2088abffc385a2ef584e&token=1495321435&lang=zh_CN&scene=21#wechat_redirect)
-   [我们为什么要分库分表？](https://mp.weixin.qq.com/s?__biz=Mzg3NzU5NTIwNg==&mid=2247498625&idx=1&sn=0d7bd9d1b46eeff4c715a6761355e9b0&chksm=cf2224a8f855adbea8931c8e011711f6c70cffeef8ddf8b87729c710eacef11b46eef80fda36&token=1712314640&lang=zh_CN#rd)





## Limit数据很大问题

###  limit 1000,10 会比 limit 10 更慢

#### 基于主键索引的limit执行过程

```sql
limit offset, size 和 limit size
其实 limit size ，相当于  limit 0, size。也就是从0开始取size条数据。

select * from page order by id limit 10000, 10
（1）在innodb里的主键索引中获取到第0到（1000 + 10）条完整行数据，返回给server层【费时的地方】
（2）server层根据offset的值挨个抛弃，最后只留下最后面的size条，也就是10条数据
```

优化

```sql
问题：用的select * 会拷贝（10000+10）条数据的完整字段，费时
解决：先查id，再根据id查整个字段

select * from page  
where id >=(select id from page  order by id limit 10000, 1) 
order by id limit 10;
```

#### 基于非主键索引的limit执行过程

```sql
select * from page order by user_name  limit 0, 10;

区别在于需要一次回表操作
当limit offset过大时，非主键索引查询非常容易变成全表扫描
```

优化

```sql
select * 
from page t1, (select id from page order by user_name limit 10000, 100) t2  
WHERE t1.id = t2.id;

问题：只是解决了回表的问题，但是还是会查询10000+100条数据给server层，再抛弃1w条数据
offset过大时，比如到了百万千万的量级，就是“深度分页问题”
```



### limit 1000000, 10 加载很慢的话，你是怎么解决的呢？

```sql
方案一: 如果 id 是连续的，返回上次查询的最大记录(偏 移量)，再往下 limit
select id，name 
from employee where id > 1000000 limit 10.

方案二：在业务允许的情况下限制页数：
建议跟业务讨论，有没有必要查这么后的分页啦。因为绝大多数用户都不会往 后翻太多页。

方案三：order by + 索引（id 为索引） ？？？感觉还是会查100w+10条
select id，name 
from employee order by id limit 1000000，10

方案四：利用延迟关联或者子查询优化超多分页场景。（先快速定位 需要获取的 id 段，然后再关联）
select a.* 
from employee a, (select id from employee where 条件 limit 100000, 10) b
where a.id = b.id
```

[参考：mysql查询 limit 1000,10 和limit 10 速度一样快吗？如果我要分页，我该怎么办？](https://mp.weixin.qq.com/s?__biz=Mzg3NzU5NTIwNg==&mid=2247499838&idx=1&sn=e30f577d2a5e4fc52e40fb070293be9d&chksm=cf221f17f85596018dde39ddfd281a527bd92c5f654d76d7313b8485d04660e559090fc0d87b&token=1712314640&lang=zh_CN#rd)



## [乐观锁 & 悲观锁](https://mp.weixin.qq.com/s?__biz=Mzg3NzU5NTIwNg==&mid=2247487996&idx=1&sn=cafae3423e5ffa903a0c0a94a355f981&source=41#wechat_redirect)

### 乐观锁

是什么

```
它认为数据的变动不会太频繁。因此，它允许多个事务同时对数据进行变动。
当事务最后提交时，系统会检查在该事务开始以后，是否有其他事务修改了它所读取或写入的数据。如果其他事务有修改，那么乐观锁策略通常会回滚事务。
```

实现方式

```
乐观锁通常是通过在表中增加一个版本(version)或时间戳(timestamp)来实现，其中，版本最为常用。

（1）版本号：为数据库表添加一个版本号字段。当读取数据时，将版本号一起读出，数据每更新一次，对应的版本号就加一。当提交事务时检查版本号是否改变，如果版本号发生了变化，说明数据已经被其他事务更新，当前事务会回滚。

（2）时间戳：与版本号类似，但是使用数据的时间戳来判断数据是否有变化。
```

使用场景

```
读多写少
```



### 悲观锁

是什么

```
是一种在数据处理前认为其他并发事务会产生冲突，所以在开始处理前先请求锁以确保在锁定期间不会有其他事务来修改数据的并发控制机制。
```

实现方式

```
SELECT FOR UPDATE
```

使用场景

```
写多读少
```



## 在高并发情况下，如何做到安全的修改同一行数据？

```
要安全的修改同一行数据，就要保证一个线程在修改时其它线程无法更新这行 记录。一般有悲观锁和乐观锁两种方案~
```



## select for update 含义，加行锁还是表锁 ？？？

```
select for update 除了有查询的作用外，还会加锁（悲观锁）
没用索引/主键的话就是表锁；否则就是是行锁，只会锁定这些索引或主键对应的行。
```

